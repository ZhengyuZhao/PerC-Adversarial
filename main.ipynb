{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "from differential_color_functions import rgb2lab_diff, ciede2000_diff\n",
    "from perc_cw import PerC_CW\n",
    "from perc_al import PerC_AL\n",
    "import differential_color_functions\n",
    "# get the list of images along with the specified target labels\n",
    "def load_ground_truth(csv_filename):\n",
    "    image_id_list = []\n",
    "    label_tar_list = []\n",
    "\n",
    "    with open(csv_filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            image_id_list.append( row['ImageId'] )\n",
    "            label_tar_list.append( int(row['TargetClass'])-1 )\n",
    "\n",
    "    return image_id_list,label_tar_list\n",
    "\n",
    "# simple Module to normalize an image\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "\n",
    "# values are standard normalization for ImageNet images, from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "trn = transforms.Compose([\n",
    "     transforms.ToTensor(),])\n",
    "\n",
    "# fix the random seed of pytorch and make cudnn deterministic for reproducing the same results\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image list\n",
    "image_id_list,label_tar_list=load_ground_truth('images.csv')\n",
    "# specify the device    \n",
    "device  = torch.device(\"cuda:0\")\n",
    "# load the pre-trained model\n",
    "model = models.inception_v3(pretrained=True,transform_input=False).eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## implement PerC_C&W and PerC_AL\n",
    "\n",
    "\n",
    "#set the mode (untargeted or targeted)\n",
    "untargeted=1\n",
    "batch_size=20\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "color_differences=[]\n",
    "l2_norm=[]\n",
    "l_inf_norm=[]\n",
    "output_path='images_adv_PerC_AL'\n",
    "if os.path.exists (output_path)==False:\n",
    "    os.mkdir(output_path)\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)\n",
    "    #load a batch of input images with the size of batch_size*channel*height*width\n",
    "    X_ori = torch.zeros(batch_size_cur,3,299,299).to(device)    \n",
    "    for i in range(batch_size_cur):  \n",
    "        X_ori[i]=trn(Image.open(os.path.join('images',image_id_list[k*batch_size+i])+'.png'))  \n",
    "    X_ori_LAB=rgb2lab_diff(X_ori,device)\n",
    "    \n",
    "    if untargeted:\n",
    "        labels=torch.argmax(model((X_ori-0.5)/0.5),dim=1)\n",
    "    \n",
    "    else:\n",
    "        labels=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "\n",
    "#     approach = PerC_CW(device=device,search_steps=9,max_iterations=1000,learning_rate=0.01,initial_const=10)\n",
    "    approach = PerC_AL(device=device,max_iterations=1000,alpha_l_init=1,alpha_c_init=0.5,confidence=40)\n",
    "\n",
    "    X_adv = approach.adversary(model, X_ori, labels=labels, targeted=False)\n",
    "\n",
    "    color_distance_map=ciede2000_diff(X_ori_LAB,rgb2lab_diff(X_adv,device),device)   \n",
    "    #claculate perceptual color_differences, L2 norm and L_inf norm\n",
    "    color_differences.append(torch.norm(color_distance_map.view(batch_size_cur,-1),dim=1).cpu().numpy())\n",
    "    l2_norm.append(torch.norm((X_adv-X_ori).view(batch_size_cur,-1),dim=1).cpu().numpy())\n",
    "    l_inf_norm.append(torch.norm((X_adv-X_ori).view(batch_size_cur,-1),dim=1, p=float('inf')).cpu().numpy())\n",
    "    \n",
    "    #save the modified images\n",
    "    for j in range(batch_size_cur):\n",
    "        x_np=transforms.ToPILImage()(X_adv[j].detach().cpu())\n",
    "        x_np.save(os.path.join(output_path,image_id_list[k*batch_size+j])+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation on adversarial strength (success rate)\n",
    "\n",
    "batch_size=20\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "cnt=0.\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,1000-k*batch_size)\n",
    "    X_ori = torch.zeros(batch_size_cur,3,299,299).to(device) \n",
    "    X_adv=torch.zeros_like(X_ori)\n",
    "    for i in range(batch_size_cur): \n",
    "        X_ori[i]=trn(Image.open('./images/'+image_id_list[k*batch_size+i]+'.png'))\n",
    "        X_adv[i]=trn(Image.open('./images_adv_PerC_AL/'+image_id_list[k*batch_size+i]+'.png'))\n",
    "    label_adv=torch.argmax(model((X_adv-0.5)/0.5),dim=1)\n",
    "\n",
    "#     # untargeted\n",
    "#     label_ori=torch.argmax(model((X_ori-0.5)/0.5),dim=1)\n",
    "#     cnt=cnt+torch.sum(label_adv!=label_ori)\n",
    "    # targeted\n",
    "    label_tar=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    cnt=cnt+torch.sum(label_adv==label_tar)\n",
    "\n",
    "print(cnt/len(image_id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation on robustness\n",
    "\n",
    "def bit_depth_red(X_before,depth):\n",
    "    r=256/(2**depth)\n",
    "    x_quan=torch.round(X_before*255/r)*r/255 \n",
    "    return x_quan\n",
    "\n",
    "def JPEG_compression(X_before,quality):\n",
    "        X_after=torch.zeros_like(X_before)\n",
    "        for j in range(X_after.size(0)):\n",
    "            x_np=transforms.ToPILImage()(X_before[j].detach().cpu())\n",
    "            x_np.save('./'+'j.jpg',quality=quality)\n",
    "            X_after[j]=trn(Image.open('./'+'j.jpg'))\n",
    "        return X_after\n",
    "    \n",
    "\n",
    "batch_size=50\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "levels=[90,80,70,60,50,40,30]#JPEG compression ratios\n",
    "# levels=[7,6,5,4,3,2]#bit depths \n",
    "num=torch.zeros(len(levels))\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,1000-k*batch_size)\n",
    "    X_ori = torch.zeros(batch_size_cur,3,299,299).to(device) \n",
    "    X_adv_before=torch.zeros_like(X_ori)\n",
    "    for i in range(batch_size_cur): \n",
    "        X_ori[i]=trn(Image.open('./images/'+image_id_list[k*batch_size+i]+'.png'))\n",
    "        X_adv_before[i]=trn(Image.open('./images_adv_PerC_AL/'+image_id_list[k*batch_size+i]+'.png'))\n",
    "\n",
    "    label_ori=torch.argmax(model((X_ori-0.5)/0.5),dim=1)\n",
    "    label_adv_before=torch.argmax(model((X_adv_before-0.5)/0.5),dim=1)\n",
    "\n",
    "    for i,value in enumerate(levels):\n",
    "#         X_adv_after=bit_depth_red(X_adv_before,value)\n",
    "        X_adv_after=JPEG_compression(X_adv_before,value)\n",
    "        for k in range(1):\n",
    "            label_after=torch.argmax(model((X_adv_after-0.5)/0.5),dim=1)\n",
    "            num[i]=num[i]+torch.sum(label_after!=label_ori)\n",
    "\n",
    "print(num/len(image_id_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation on tranferability\n",
    "\n",
    "model2 = models.googlenet(pretrained=True,transform_input=False).eval()\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad=False\n",
    "device  = torch.device(\"cuda:0\")\n",
    "model2.to(device)\n",
    "\n",
    "model3 = models.vgg16_bn(pretrained=True).eval()\n",
    "for param in model3.parameters():\n",
    "    param.requires_grad=False\n",
    "device  = torch.device(\"cuda:0\")\n",
    "model3.to(device)\n",
    "\n",
    "model4 = models.resnet152(pretrained=True).eval()\n",
    "for param in model4.parameters():\n",
    "    param.requires_grad=False\n",
    "device  = torch.device(\"cuda:0\")\n",
    "model4.to(device)\n",
    "\n",
    "\n",
    "#find eligible images for tranferability\n",
    "batch_size=10\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "eligible_list=[]\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)\n",
    "    X_ori = torch.zeros(batch_size_cur,3,299,299).to(device) \n",
    "    for i in range(batch_size_cur): \n",
    "        X_ori[i]=trn(Image.open('./images/'+image_id_list[k*batch_size+i]+'.png'))\n",
    "\n",
    "    pre_ori_1=torch.argmax(model((X_ori-0.5)/0.5),dim=1)\n",
    "    pre_ori_2=torch.argmax(model2((X_ori-0.5)/0.5),dim=1)\n",
    "    pre_ori_3=torch.argmax(model3(norm(X_ori)),dim=1)\n",
    "    pre_ori_4=torch.argmax(model4(norm(X_ori)),dim=1)\n",
    "    mask=((pre_ori_1==pre_ori_2).float() * (pre_ori_1==pre_ori_3).float() * (pre_ori_1==pre_ori_4).float() * (pre_ori_2==pre_ori_3).float() * (pre_ori_2==pre_ori_4).float() * (pre_ori_3==pre_ori_4).float())\n",
    "    eligible_list.append(mask)\n",
    "image_transfer_list=[]\n",
    "for i in range(len(eligible_list)):\n",
    "    for j in range(batch_size):\n",
    "        if eligible_list[i][j]==1:\n",
    "            image_transfer_list.append(image_id_list[i*batch_size+j])\n",
    "\n",
    "#calculate tranferability\n",
    "num_batches = np.int(np.ceil(len(image_transfer_list)/batch_size))\n",
    "eligible_list=[]\n",
    "adv_folder='./images_cw_untar_9000_1_k_40/'\n",
    "sr_1=0\n",
    "sr_2=0\n",
    "sr_3=0\n",
    "sr_4=0\n",
    "\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_transfer_list)-k*batch_size)\n",
    "    X_ori = torch.zeros(batch_size_cur,3,299,299).to(device) \n",
    "    x_adv = torch.zeros(batch_size_cur,3,299,299).to(device) \n",
    "    for i in range(batch_size_cur): \n",
    "        X_ori[i]=trn(Image.open('./images/'+image_transfer_list[k*batch_size+i]+'.png'))\n",
    "        x_adv[i]=trn(Image.open(adv_folder+image_transfer_list[k*batch_size+i]+'.png'))\n",
    "    pre_ori=torch.argmax(model((X_ori-0.5)/0.5),dim=1)\n",
    "    \n",
    "    pre_adv=torch.argmax(model((x_adv-0.5)/0.5),dim=1)\n",
    "    pre_adv_2=torch.argmax(model2((x_adv-0.5)/0.5),dim=1)\n",
    "    pre_adv_3=torch.argmax(model3(norm(X_ori)),dim=1)\n",
    "    pre_adv_4=torch.argmax(model4(norm(X_ori)),dim=1)\n",
    "    sr_1=sr_1+sum((pre_ori!=pre_adv).float())\n",
    "    sr_2=sr_2+sum((pre_ori!=pre_adv_2).float())\n",
    "    sr_3=sr_3+sum((pre_ori!=pre_adv_3).float())\n",
    "    sr_4=sr_4+sum((pre_ori!=pre_adv_4).float())\n",
    "print(sr_1/len(image_transfer_list),sr_2/len(image_transfer_list),sr_3/len(image_transfer_list),sr_4/len(image_transfer_list))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
